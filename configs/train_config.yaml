# Training Configuration for WildGuardMix Classifier

# Global settings
seed: &seed 42

# Model Configuration
model:
  name: "answerdotai/ModernBERT-large"
  max_length: 2048
  num_labels: 2
  problem_type: "single_label_classification"

# Dataset Configuration
dataset:
  subset: "wildguardtrain"
  test_size: 0.8
  val_size: 0.1
  random_state: *seed
  label_column: "prompt_harm_label"
  text_column: "prompt"
  positive_label: "harmful"
  num_proc: 8  # Number of workers for tokenization
  use_label_weights: true  # Weight by harm label frequency (harmful vs unharmful)
  use_subcategory_weights: false  # Weight by subcategory frequency
  class_weight_column: "subcategory"  # Column to compute subcategory weights from

ensemble:
  enabled: True
  num_models: 3

# Training Arguments (passed directly to TrainingArguments)
training:
  output_dir: "models/modernbert-ensemble"
  num_train_epochs: 1
  per_device_train_batch_size: 48
  per_device_eval_batch_size: 48
  learning_rate: 1e-4
  lr_scheduler_type: "cosine"
  warmup_ratio: 0.1
  weight_decay: 1e-5
  adam_beta1: 0.9
  adam_beta2: 0.98
  adam_epsilon: 1e-6
  
  # Optimization
  optim: "adamw_bnb_8bit"
  torch_compile: true
  
  # Evaluation
  eval_strategy: "steps"
  eval_steps: 0.2  # Every 20% of epoch
  save_strategy: "epoch"
  save_total_limit: 2
  load_best_model_at_end: false
  
  # Logging
  logging_dir: "logs"
  logging_steps: 10
  report_to: "wandb"
  log_level: "warning"
  
  # DataLoader
  dataloader_num_workers: 8
  dataloader_pin_memory: true
  
  # Misc
  remove_unused_columns: true
  push_to_hub: false
  seed: *seed

# WandB Configuration
wandb:
  entity: "intention-analysis"
  project: "classifier-ensemble"
  dir: "logs/wandb"
  run_name: null
  tags:
    - "modernbert"
    - "wildguardmix"
    - "binary-classification"

# Hydra Configuration
hydra:
  run:
    dir: logs/hydra/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: logs/hydra/multirun/${now:%Y-%m-%d}/${now:%H-%M-%S}
